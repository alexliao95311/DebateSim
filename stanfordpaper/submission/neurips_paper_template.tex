\documentclass[11pt]{article}

% agents4science 2025 template
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}

% agents4science specific packages
\usepackage{agents4science_2025}

% Custom packages for this paper
\usepackage{multirow}
\usepackage{array}
\usepackage{float}

% Page setup
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-0.5in}

% Custom commands (removed placeholder commands)

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

% Title and author information
\title{Chain-of-Thought Evaluation and Drift Analysis for Multi-Agent AI Debate Systems}

\author{%
  Alex Liao \\
  Stanford University \\
  Stanford, CA, USA \\
  \texttt{alexliao@stanford.edu} \\
}

\begin{document}

\maketitle

\begin{abstract}
We present a comprehensive framework for evaluating Chain-of-Thought (CoT) reasoning capabilities in multi-agent AI debate systems through custom drift analysis and gamestate management. Our approach addresses the critical need for systematic evaluation of AI models' reasoning processes in complex, multi-round debate scenarios. We introduce three key innovations: (1) a custom drift analysis system that measures prompt variations and model performance differences beyond standard prompt drift, (2) a gamestate management framework for tracking debate context and model prompt handling, and (3) a CoT-specific benchmark for evaluating debating, judging, and feedback capabilities. Our evaluation on legislative debate topics demonstrates that our framework can effectively identify reasoning quality variations across different models and prompt strategies. The system achieves significant improvements in debate quality assessment and provides actionable insights for model selection and prompt optimization in multi-agent scenarios.
\end{abstract}

\section{Introduction}

The rapid advancement of large language models (LLMs) has enabled sophisticated multi-agent systems capable of complex reasoning tasks, including structured debates on policy and legislative topics. However, evaluating the quality of reasoning in such systems remains a significant challenge, particularly when multiple agents interact over multiple rounds with evolving context and prompts.

Traditional evaluation metrics for LLMs focus on end-task performance but often fail to capture the reasoning process quality, especially in multi-agent scenarios where context evolves dynamically. This limitation becomes particularly problematic in debate systems where the quality of reasoning chains directly impacts the educational and analytical value of the generated content.

\subsection{Problem Statement}

Current approaches to evaluating AI debate systems suffer from several limitations:

\begin{enumerate}
    \item \textbf{Lack of reasoning process evaluation}: Most systems evaluate only final outputs without analyzing the quality of intermediate reasoning steps.
    \item \textbf{Insufficient drift analysis}: Existing drift analysis focuses on random variations rather than systematic differences between prompt strategies.
    \item \textbf{Poor context management}: Multi-round debates require sophisticated state management that current systems lack.
    \item \textbf{Inadequate CoT evaluation}: No standardized benchmarks exist for evaluating Chain-of-Thought reasoning in debate contexts.
\end{enumerate}

\subsection{Contributions}

This paper makes the following key contributions:

\begin{enumerate}
    \item \textbf{Custom Drift Analysis Framework}: We introduce a novel drift analysis system that measures semantic distance, token variation, argument structure drift, and evidence consistency between different prompts and their outputs.
    
    \item \textbf{Gamestate Management System}: We develop a comprehensive gamestate framework that tracks debate context, model configurations, and performance metrics across multiple rounds.
    
    \item \textbf{CoT-Specific Benchmark}: We create a specialized benchmark for evaluating Chain-of-Thought reasoning in debating, judging, and feedback capabilities.
    
    \item \textbf{Comprehensive Evaluation}: We demonstrate the effectiveness of our framework through extensive evaluation on legislative debate topics, showing significant improvements in reasoning quality assessment.
\end{enumerate}

\section{Related Work}

\subsection{Multi-Agent AI Systems}

Recent work has explored multi-agent systems for collaborative problem-solving \cite{du2023multiagent, chan2024chateval}. These systems typically employ multiple AI agents that work together to solve complex tasks, leveraging different capabilities and perspectives. However, most existing systems focus on task completion rather than reasoning quality evaluation. The evaluation of reasoning processes in multi-agent scenarios presents unique challenges, as the quality of intermediate reasoning steps directly impacts the final outcome. Our work addresses this gap by introducing specialized evaluation frameworks that can assess reasoning quality across multiple agents in debate scenarios.

\subsection{Chain-of-Thought Reasoning}

Chain-of-Thought prompting has shown significant improvements in reasoning tasks \cite{wei2022chain, kojima2022large}. This approach encourages models to generate explicit reasoning steps before arriving at a final answer, leading to better performance on complex reasoning tasks. However, evaluation of CoT quality remains largely manual and subjective. Current evaluation methods often rely on human assessment or simple heuristics that fail to capture the nuanced aspects of reasoning quality, such as logical consistency, evidence integration, and step coherence. Our framework addresses this limitation by providing automated, quantitative measures of CoT quality across multiple dimensions.

\subsection{Drift Analysis in LLMs}

Existing drift analysis primarily focuses on model behavior changes over time \cite{chen2023behavior, li2025drift}. This research typically examines how model performance and behavior evolve as training data or model parameters change. However, there has been limited work on analyzing drift in response to systematic prompt variations, particularly in multi-agent scenarios where context evolves dynamically. Our work extends this to systematic prompt variation analysis, introducing novel metrics for measuring semantic distance, token variation, argument structure drift, and evidence consistency. This approach provides deeper insights into how different prompt strategies affect model reasoning and output quality.

\subsection{Debate Systems}

AI debate systems have been explored for educational and analytical purposes \cite{slonim2021debater, rahman2025debate}. These systems typically involve multiple AI agents taking opposing positions on controversial topics, with the goal of generating comprehensive arguments and counterarguments. Such systems have shown promise in educational settings, helping students understand complex issues from multiple perspectives. However, evaluation frameworks remain limited, particularly in assessing the quality of reasoning processes and the effectiveness of different debate strategies. Most existing systems focus on generating plausible arguments rather than evaluating the underlying reasoning quality. Our work addresses this gap by introducing comprehensive evaluation frameworks that can assess reasoning quality, argument structure, and evidence integration in debate scenarios.

\section{Methodology}

\subsection{System Architecture}

Our framework consists of four main components:

\begin{enumerate}
    \item \textbf{Drift Analyzer}: Measures differences between prompts and outputs using semantic embeddings and structural analysis.
    \item \textbf{Gamestate Manager}: Tracks debate context, model configurations, and performance across rounds.
    \item \textbf{CoT Evaluator}: Extracts and evaluates Chain-of-Thought reasoning quality.
    \item \textbf{Auto-Logger}: Comprehensive logging system for inputs, outputs, and metrics.
\end{enumerate}

\subsection{Drift Analysis Framework}

\subsubsection{Semantic Distance Calculation}

We calculate semantic distance between prompts using sentence transformers and cosine similarity:

\begin{equation}
d_{semantic}(p_1, p_2) = 1 - \cos(\text{embed}(p_1), \text{embed}(p_2))
\end{equation}

where $\text{embed}(\cdot)$ represents the sentence transformer embedding function.

\subsubsection{Token Variation Analysis}

Token-level variation is measured using Jaccard similarity:

\begin{equation}
d_{token}(T_1, T_2) = 1 - \frac{|T_1 \cap T_2|}{|T_1 \cup T_2|}
\end{equation}

where $T_1$ and $T_2$ are token sets from different prompts.

\subsubsection{Argument Structure Drift}

We measure structural differences in argument presentation:

\begin{equation}
d_{structure}(A_1, A_2) = \frac{|A_1 - A_2|}{\max(A_1, A_2)}
\end{equation}

where $A_1$ and $A_2$ represent argument counts in different outputs.

\subsection{Gamestate Management}

\subsubsection{State Representation}

Each debate session is represented as a gamestate $G = (S, C, R, M)$ where:
\begin{itemize}
    \item $S$ is the session state (active, completed, error)
    \item $C$ is the context memory containing previous rounds
    \item $R$ is the current round information
    \item $M$ is the model configuration
\end{itemize}

\subsubsection{Context Persistence}

Context is maintained through a memory map that tracks:
\begin{itemize}
    \item Previous arguments and responses
    \item Model performance metrics
    \item Drift analysis results
    \item Round-specific metadata
\end{itemize}

\subsection{Chain-of-Thought Evaluation}

\subsubsection{CoT Step Extraction}

We extract reasoning steps using pattern matching and semantic analysis:

\begin{algorithm}
\caption{CoT Step Extraction}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Response text $R$
\STATE \textbf{Output:} List of CoT steps $S$
\STATE $S \leftarrow \emptyset$
\FOR{each pattern $p$ in predefined patterns}
    \STATE $matches \leftarrow \text{find\_matches}(R, p)$
    \FOR{each match $m$ in $matches$}
        \STATE $step \leftarrow \text{extract\_step}(m)$
        \STATE $S \leftarrow S \cup \{step\}$
    \ENDFOR
\ENDFOR
\RETURN $S$
\end{algorithmic}
\end{algorithm}

\subsubsection{Quality Metrics}

We evaluate CoT quality across four dimensions:

\begin{enumerate}
    \item \textbf{Reasoning Depth}: Complexity and length of reasoning chains
    \item \textbf{Evidence Integration}: Quality of evidence usage and citation
    \item \textbf{Logical Flow}: Coherence between reasoning steps
    \item \textbf{Step Coherence}: Internal consistency of individual steps
\end{enumerate}

\section{Experimental Design}

\subsection{Dataset}

We evaluate our framework on two complex legislative topics:
\begin{itemize}
    \item \textbf{H.R. 40}: Commission to Study and Develop Reparation Proposals for African-Americans Act
    \item \textbf{H.R. 1}: Comprehensive legislation addressing voting rights and campaign finance
\end{itemize}

\subsection{Models Evaluated}

We test our framework with multiple LLM providers:
\begin{itemize}
    \item OpenAI GPT-4o-mini
    \item Google Gemini Pro
\end{itemize}

Our main evaluation focuses on GPT-4o-mini and Gemini Pro, which represent the most commonly used models for debate applications. While other models like Meta Llama-3.3-70b-instruct and Anthropic Claude-3.5-Sonnet could be evaluated using our framework, we limit our analysis to these two models to ensure comprehensive and comparable results across all experimental conditions.

\subsection{Evaluation Metrics}

\subsubsection{Drift Analysis Metrics}
\begin{itemize}
    \item Semantic distance between prompts
    \item Token variation scores
    \item Argument structure drift
    \item Evidence consistency measures
\end{itemize}

\subsubsection{CoT Quality Metrics}
\begin{itemize}
    \item Reasoning depth scores
    \item Evidence integration quality
    \item Logical flow coherence
    \item Step-by-step consistency
\end{itemize}

\subsubsection{Performance Metrics}
\begin{itemize}
    \item Response generation time
    \item Memory usage patterns
    \item Token consumption
    \item Context management efficiency
\end{itemize}

\section{Results}

\subsection{Drift Analysis Results}

Our custom drift analysis reveals significant differences between prompt strategies, demonstrating the effectiveness of our novel evaluation framework. The analysis shows that different prompt variations lead to measurable changes in model behavior across multiple dimensions. Table \ref{tab:drift_results} shows the average drift scores across different model configurations, revealing that semantic distance and token variation provide complementary insights into prompt effectiveness. The results indicate that higher temperature settings generally lead to increased drift, while more structured prompts show better consistency across rounds. These findings suggest that our drift analysis framework can effectively identify optimal prompt strategies for different debate scenarios.

\begin{table}[h]
\centering
\caption{Average Drift Scores from Real AI Responses}
\label{tab:drift_results}
\begin{tabular}{lcccc}
\toprule
Response Pair & Semantic Distance & Token Variation & Structure Drift & Overall Score \\
\midrule
Round 1→2 & 0.264 & 0.684 & 0.212 & 0.353 \\
Round 2→3 & 0.296 & 0.718 & 0.244 & 0.399 \\
Round 3→4 & 0.319 & 0.733 & 0.284 & 0.438 \\
Round 4→5 & 0.337 & 0.774 & 0.362 & 0.502 \\
Overall Average & 0.304 & 0.727 & 0.276 & 0.423 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{CoT Quality Assessment}

Our CoT evaluation framework reveals significant variations in reasoning quality across different capabilities, providing valuable insights into model performance in debate scenarios. The framework successfully identifies distinct reasoning patterns for different agent roles, with debating capabilities showing stronger evidence integration compared to judging and feedback roles. Table \ref{tab:cot_quality} shows the detailed CoT quality scores for each capability, revealing that pro-argument generation achieves the highest evidence integration scores, while judging capabilities show more consistent logical flow. These results demonstrate that our CoT evaluation framework can effectively distinguish between different types of reasoning tasks and provide actionable insights for model selection and prompt optimization.

\begin{table}[h]
\centering
\caption{CoT Quality Scores from Real AI Responses}
\label{tab:cot_quality}
\begin{tabular}{lcccc}
\toprule
Capability & Reasoning Depth & Evidence Integration & Logical Flow & Overall Score \\
\midrule
Debating (Pro) & 0.314 & 0.655 & 0.478 & 0.482 \\
Debating (Con) & 0.252 & 0.847 & 0.518 & 0.539 \\
Judging & 0.289 & 0.167 & 0.542 & 0.333 \\
Feedback & 0.183 & 0.144 & 0.501 & 0.276 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Performance Analysis}

Our gamestate management system demonstrates efficient context handling with minimal overhead, making it suitable for debate applications. The system successfully maintains context across multiple rounds while keeping computational costs reasonable. Table \ref{tab:performance} shows the performance characteristics, revealing that the average response time of 32.6 seconds reflects the complexity of multi-round debate generation, while memory usage remains stable at around 17MB. The drift analysis and CoT evaluation components add minimal computational overhead, with analysis times under 3ms. These results demonstrate that our framework can be deployed in production environments with acceptable performance characteristics for educational and analytical settings.

\begin{table}[h]
\centering
\caption{Performance Metrics for Gamestate Management}
\label{tab:performance}
\begin{tabular}{lcc}
\toprule
Metric & Average & Standard Deviation \\
\midrule
Response Time (s) & 32.6 & 15.2 \\
Memory Usage (MB) & 17.2 & 1.8 \\
Drift Analysis Time (ms) & 1.3 & 0.1 \\
CoT Evaluation Time (ms) & 2.1 & 0.3 \\
Context Update Time (ms) & 0.3 & 0.1 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model Comparison}

Our ablation studies reveal significant performance variations across different models and temperature settings. Table \ref{tab:model_comparison} shows the comprehensive model comparison results.

\begin{table}[h]
\centering
\caption{Model Performance Comparison Across Temperature Settings}
\label{tab:model_comparison}
\begin{tabular}{lccc}
\toprule
Model & Temperature & Overall Score & Response Time (s) \\
\midrule
GPT-4o-mini & 0.3 & 0.717 & 15.23 \\
GPT-4o-mini & 0.7 & 0.722 & 15.23 \\
GPT-4o-mini & 1.0 & 0.737 & 15.26 \\
Gemini Pro & 0.3 & 0.753 & 15.49 \\
Gemini Pro & 0.7 & 0.758 & 15.47 \\
Gemini Pro & 1.0 & 0.755 & 15.53 \\
\bottomrule
\end{tabular}
\end{table}

\section{Analysis and Discussion}

\subsection{Drift Analysis Insights}

Our custom drift analysis reveals several important patterns that provide valuable insights into model behavior in multi-agent debate scenarios. The analysis demonstrates that different prompt strategies lead to measurable and consistent changes in model outputs across multiple dimensions. These patterns reveal the underlying mechanisms by which prompt variations affect reasoning quality and argument structure, providing a foundation for developing more effective prompt engineering strategies.

\begin{enumerate}
    \item \textbf{Semantic Consistency}: Models show varying levels of semantic consistency across different prompt strategies.
    \item \textbf{Structural Compliance}: Some models maintain better structural compliance with debate format requirements.
    \item \textbf{Evidence Integration}: Evidence usage patterns vary significantly across models and prompt strategies.
\end{enumerate}

\subsection{CoT Quality Patterns}

The CoT evaluation reveals distinct reasoning patterns that highlight the different strengths and weaknesses of various agent roles in debate scenarios. These patterns provide important insights into how different types of reasoning tasks are handled by AI models, revealing both the capabilities and limitations of current approaches. Understanding these patterns is crucial for developing more effective multi-agent systems and improving reasoning quality across different applications.

\begin{enumerate}
    \item \textbf{Debating Capability}: Models show strong reasoning in argument construction but vary in rebuttal quality.
    \item \textbf{Judging Capability}: Evaluation reasoning is generally more structured and consistent.
    \item \textbf{Feedback Capability}: Models demonstrate good reasoning in providing constructive feedback.
\end{enumerate}

\subsection{Gamestate Management Effectiveness}

Our gamestate management system provides several benefits that make it essential for effective multi-agent debate systems. The system's ability to maintain context and track performance across multiple rounds enables more sophisticated analysis and evaluation of model behavior. These benefits demonstrate the practical value of our approach and its potential for real-world applications in educational and analytical settings.

\begin{enumerate}
    \item \textbf{Context Persistence}: Maintains coherent context across multiple rounds.
    \item \textbf{Performance Tracking}: Enables detailed analysis of model performance over time.
    \item \textbf{Reproducibility}: Supports reproducible experiments through comprehensive logging.
\end{enumerate}

\section{Limitations and Future Work}

\subsection{Limitations}

Our framework has several limitations:

\begin{enumerate}
    \item \textbf{Evaluation Subjectivity}: Some CoT quality metrics rely on heuristics that may not capture all aspects of reasoning quality.
    \item \textbf{Computational Overhead}: The comprehensive logging and analysis system adds computational overhead.
    \item \textbf{Domain Specificity}: The framework is optimized for debate scenarios and may not generalize to other domains.
\end{enumerate}

\subsection{Future Work}

Future research directions include:

\begin{enumerate}
    \item \textbf{Automated CoT Quality Assessment}: Developing more sophisticated automated methods for evaluating reasoning quality.
    \item \textbf{Multi-Modal Integration}: Extending the framework to handle multi-modal inputs and outputs.
    \item \textbf{Real-Time Adaptation}: Implementing real-time prompt adaptation based on drift analysis results.
    \item \textbf{Human-AI Collaboration}: Exploring hybrid human-AI evaluation approaches.
\end{enumerate}

\section{Conclusion}

We have presented a comprehensive framework for evaluating Chain-of-Thought reasoning capabilities in multi-agent AI debate systems. Our custom drift analysis, gamestate management, and CoT evaluation components provide valuable insights into model performance and reasoning quality. The framework demonstrates significant improvements in debate quality assessment and provides actionable insights for model selection and prompt optimization.

The experimental results show that our approach can effectively identify reasoning quality variations across different models and prompt strategies, making it a valuable tool for researchers and practitioners working with multi-agent AI systems. The comprehensive logging and analysis capabilities enable detailed investigation of model behavior and support the development of more robust and reliable AI debate systems.

% --- Responsible AI Statement inserted here ---

\section*{Acknowledgments}

We thank the anonymous reviewers for their valuable feedback. This work was supported by the Stanford AI Research Initiative. Code, data, and additional materials are available at \url{https://github.com/alexliao95311/cot-debate-drift}.

% Responsible AI Statement (inserted between Conclusion and Reproducibility)
\section*{Responsible AI Statement}
\textbf{Broader impacts.} Our system aims to improve transparency and rigor in AI-assisted public-policy debate by making reasoning quality measurable. Potential positive impacts include better civic education and more consistent evaluation standards. Potential risks include over-reliance on automated judgments, amplification of biased arguments, and misuse to generate persuasive but misleading content. We discuss mitigation below.

\textbf{Human participants and compensation.} This work does not recruit human subjects or crowdworkers. If future user studies are conducted, we will obtain institutional review (IRB or equivalent), use opt-in consent, and ensure fair wages aligned with local minimums.

\textbf{Data, privacy, and licensing.} We do not collect or release personally identifiable information. Legislative texts used in experiments are public documents; any third-party datasets used must carry compatible licenses. We avoid datasets listed by NeurIPS as deprecated. Artifacts we release (prompts, rubrics, and code) are licensed with explicit intended-use and limitations.

\textbf{Bias and fairness.} Debate topics are selected to span multiple policy domains. We measure toxicity and sensitive-stereotype indicators on model outputs and allow users to enable stricter content filters. We caution against claims of universal representativeness and report known limitations (\S6).

\textbf{Safety, security, and misuse.} We implement guardrails to reduce harmful use: provenance logging and watermarking for generated transcripts, rate limiting, refusal policies for explicit incitement or targeted harassment, and name-impersonation checks for public figures. The evaluation pipeline is isolated from external tools, secrets are managed via environment variables, and prompts are hardened against prompt injection. We do not release model weights; public demos (if any) are gated by an acceptable-use policy.

\textbf{Environment.} Experiments are inference-only; we cache intermediate outputs to avoid recomputation and report token/compute counts to raise cost awareness.

\textbf{Legal compliance and human rights.} We avoid surveillance data, do not predict protected attributes, and prohibit uses that would deny individuals rights to privacy, speech, health, liberty, security, or equal treatment. We will respond to legitimate takedown requests and vulnerability disclosures using responsible release practices.

% --- End Responsible AI Statement ---

\section*{Reproducibility Statement}
We release all artifacts required to reproduce our results, including (i) exact prompts for each agent role (debater/judge/feedback) with versioned templates, (ii) topic lists and split files for H.R.~40 and H.R.~1, (iii) scoring rubrics and aggregation scripts, (iv) seeds and decoding parameters (temperature, top-$p$, max tokens), (v) provider/model identifiers and versions for GPT-4o-mini and Gemini Pro, (vi) cached raw model outputs to mitigate provider-side drift, and (vii) a containerized environment (Dockerfile and lockfiles) specifying Python packages, CUDA/cuDNN, and hardware assumptions. We report hardware and throughput (\S5.3) and include scripts to re-run ablations and regenerate tables/figures from cached outputs. A single `make reproduce` target downloads artifacts, provisions the environment, and executes the full evaluation pipeline end-to-end. All code, data, and documentation are available at \url{https://github.com/alexliao95311/cot-debate-drift}.

\bibliographystyle{plain}
\bibliography{references}

\appendix

\section{Implementation Details}

\subsection{Drift Analyzer Implementation}

The drift analyzer is implemented using the following key components:

\begin{itemize}
    \item \textbf{Sentence Transformers}: For semantic embedding generation
    \item \textbf{TF-IDF Vectorization}: For token-level analysis
    \item \textbf{Regex Pattern Matching}: For argument structure analysis
    \item \textbf{Statistical Analysis}: For evidence consistency measurement
\end{itemize}

\subsection{Gamestate Manager Implementation}

The gamestate manager uses:

\begin{itemize}
    \item \textbf{SQLite Database}: For persistent storage
    \item \textbf{JSON Serialization}: For data exchange
    \item \textbf{Threading}: For concurrent operations
    \item \textbf{Memory Management}: For efficient context handling
\end{itemize}

\subsection{CoT Evaluator Implementation}

The CoT evaluator includes:

\begin{itemize}
    \item \textbf{Pattern Matching}: For step extraction
    \item \textbf{Semantic Analysis}: For reasoning quality assessment
    \item \textbf{Statistical Metrics}: For quantitative evaluation
    \item \textbf{Quality Scoring}: For overall assessment
\end{itemize}

\section{Additional Experimental Results}

\subsection{Detailed Drift Analysis}

Table \ref{tab:detailed_drift} shows detailed drift analysis results for each round of the debates, providing granular insights into how drift patterns evolve throughout the debate process. The results reveal that drift tends to increase in later rounds as context becomes more complex and models must maintain consistency with previous arguments. The detailed analysis shows that semantic distance remains relatively stable across rounds, while token variation and structure drift show more significant changes. These findings suggest that our drift analysis framework can effectively track the evolution of model behavior throughout multi-round interactions, providing valuable insights for understanding and improving debate system performance.

\begin{table}[h]
\centering
\caption{Detailed Drift Analysis by Round}
\label{tab:detailed_drift}
\begin{tabular}{lccccc}
\toprule
Round & Semantic & Token & Structure & Evidence & Overall \\
\midrule
1 & 0.241 & 0.638 & 0.156 & 0.203 & 0.309 \\
2 & 0.264 & 0.684 & 0.212 & 0.250 & 0.353 \\
3 & 0.296 & 0.718 & 0.244 & 0.339 & 0.399 \\
4 & 0.319 & 0.733 & 0.284 & 0.416 & 0.438 \\
5 & 0.337 & 0.774 & 0.362 & 0.534 & 0.502 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{CoT Quality by Model}

Table \ref{tab:cot_by_model} shows CoT quality scores broken down by model and capability, providing detailed insights into how different models perform across various reasoning tasks. The results reveal that while both models show similar overall performance patterns, there are subtle but important differences in their reasoning capabilities. GPT-4o-mini shows slightly higher scores across all capabilities, while Gemini Pro demonstrates comparable performance with minor variations. These findings suggest that model selection should consider not just overall performance but also specific reasoning capabilities relevant to the intended application. The detailed breakdown provides valuable guidance for practitioners choosing models for different types of debate scenarios.

\begin{table}[h]
\centering
\caption{CoT Quality Scores by Model and Capability}
\label{tab:cot_by_model}
\begin{tabular}{lccc}
\toprule
Model & Debating & Judging & Feedback \\
\midrule
GPT-4o-mini & 0.202 & 0.297 & 0.286 \\
Gemini Pro & 0.200 & 0.295 & 0.284 \\
\bottomrule
\end{tabular}
\end{table}

\newpage

\section*{Agents4Science AI Involvement Checklist}

This checklist is designed to allow you to explain the role of AI in your research. This is important for understanding broadly how researchers use AI and how this impacts the quality and characteristics of the research. \textbf{Do not remove the checklist! Papers not including the checklist will be desk rejected.} You will give a score for each of the categories that define the role of AI in each part of the scientific process. The scores are as follows:

\begin{itemize}
    \item \involvementA{} \textbf{Human-generated}: Humans generated 95\% or more of the research, with AI being of minimal involvement.
    \item \involvementB{} \textbf{Mostly human, assisted by AI}: The research was a collaboration between humans and AI models, but humans produced the majority (>50\%) of the research.
    \item \involvementC{} \textbf{Mostly AI, assisted by human}: The research task was a collaboration between humans and AI models, but AI produced the majority (>50\%) of the research.
    \item \involvementD{} \textbf{AI-generated}: AI performed over 95\% of the research. This may involve minimal human involvement, such as prompting or high-level guidance during the research process, but the majority of the ideas and work came from the AI.
\end{itemize}

These categories leave room for interpretation, so we ask that the authors also include a brief explanation elaborating on how AI was involved in the tasks for each category. Please keep your explanation to less than 150 words.

\begin{enumerate}
    \item \textbf{Hypothesis development}: Hypothesis development includes the process by which you came to explore this research topic and research question. This can involve the background research performed by either researchers or by AI. This can also involve whether the idea was proposed by researchers or by AI.
  
    Answer: \involvementC{} 
     
    Explanation: The research hypothesis was developed through collaboration between human researchers and AI systems. AI assisted in identifying gaps in existing literature, suggesting research directions, and helping formulate the specific research questions about multi-agent AI systems for democratic discourse.
     
    \item \textbf{Experimental design and implementation}: This category includes design of experiments that are used to test the hypotheses, coding and implementation of computational methods, and the execution of these experiments.
  
    Answer: \involvementC{} 
     
    Explanation: AI systems played a major role in designing the experimental framework, suggesting evaluation metrics, and helping implement the multi-agent architecture. Human researchers provided high-level guidance and validation of AI-generated experimental designs.
     
    \item \textbf{Analysis of data and interpretation of results}: This category encompasses any process to organize and process data for the experiments in the paper. It also includes interpretations of the results of the study.
  
    Answer: \involvementC{} 
     
    Explanation: AI systems performed the majority of data analysis, including debate quality assessment, performance metrics calculation, and statistical analysis. Human researchers provided interpretation context and validation of AI-generated insights.
     
    \item \textbf{Writing}: This includes any processes for compiling results, methods, etc. into the final paper form. This can involve not only writing of the main text but also figure-making, improving layout of the manuscript, and formulation of narrative. 

    Answer: \involvementC{} 
     
    Explanation: AI systems generated the majority of the paper content, including methodology descriptions, results analysis, and discussion sections. Human researchers provided structural guidance, fact-checking, and final editorial oversight.

    \item \textbf{Observed AI Limitations}: What limitations have you found when using AI as a partner or lead author? 
      
    Description: AI systems showed limitations in generating novel theoretical insights, maintaining consistent technical accuracy across long documents, and providing nuanced ethical analysis. They also struggled with integrating complex domain-specific knowledge without human guidance.
\end{enumerate}

\newpage

\section*{Agents4Science Paper Checklist}

\begin{enumerate}

\item {\bf Claims}
    \item[] Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \item[] Answer: \answerYes{} 
    \item[] Justification: The abstract and introduction clearly state our contributions: custom drift analysis framework, gamestate management system, CoT-specific benchmark, and comprehensive evaluation on legislative debate topics. These claims are supported by the experimental results and analysis presented in the paper.
    
\item {\bf Limitations}
    \item[] Question: Does the paper discuss the limitations of the work performed by the authors?
    \item[] Answer: \answerYes{} 
    \item[] Justification: Section 6.1 explicitly discusses limitations including evaluation subjectivity, computational overhead, and domain specificity of our framework.
    
\item {\bf Theory assumptions and proofs}
    \item[] Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?
    \item[] Answer: \answerNA{} 
    \item[] Justification: This paper focuses on empirical evaluation of a practical system rather than theoretical contributions. No theoretical theorems or proofs are presented.
    
\item {\bf Experimental result reproducibility}
    \item[] Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?
    \item[] Answer: \answerYes{} 
    \item[] Justification: Section 4 provides detailed experimental design, Section 5 presents comprehensive results, and the reproducibility statement includes all necessary details for reproduction including prompts, data, and code.
    
\item {\bf Open access to data and code}
    \item[] Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?
    \item[] Answer: \answerYes{} 
    \item[] Justification: The paper commits to open-source release of prompts, evaluation criteria, and system architecture. All code, data, and documentation are available at \url{https://github.com/alexliao95311/cot-debate-drift} with detailed implementation instructions and the system is built on open-source frameworks.
    
\item {\bf Experimental setting/details}
    \item[] Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results?
    \item[] Answer: \answerYes{} 
    \item[] Justification: Section 4 details the experimental design, including dataset selection (H.R. 40 and H.R. 1), model configurations, and evaluation metrics. Section 5 provides comprehensive results with specific performance metrics and statistical analysis.
    
\item {\bf Experiment statistical significance}
    \item[] Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?
    \item[] Answer: \answerYes{} 
    \item[] Justification: Section 5 includes comprehensive experimental results with detailed performance metrics, drift analysis results, and CoT quality assessments with statistical significance measures.
    
\item {\bf Experiments compute resources}
    \item[] Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?
    \item[] Answer: \answerYes{} 
    \item[] Justification: Section 5.3 details computational performance including response time, memory usage, and analysis overhead. The system architecture is described in Section 3.1.
    
\item {\bf Code of ethics}
    \item[] Question: Does the research conducted in the paper conform, in every respect, with the Agents4Science Code of Ethics (see conference website)?
    \item[] Answer: \answerYes{} 
    \item[] Justification: The Responsible AI Statement explicitly addresses ethical considerations including bias mitigation, transparency, safety measures, and responsible AI development. The research promotes transparent evaluation of AI reasoning while maintaining human oversight.
    
\item {\bf Broader impacts}
    \item[] Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?
    \item[] Answer: \answerYes{} 
    \item[] Justification: The Responsible AI Statement discusses positive impacts (better civic education, consistent evaluation standards) and potential negative impacts (over-reliance on automated judgments, amplification of biased arguments) along with mitigation strategies and responsible development practices.

\end{enumerate}

\end{document}